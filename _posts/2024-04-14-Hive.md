---
layout: post
title: Hive
tags: [Hive]
categories: Demo
excerpt_separator: <!--more-->
---

Hive
<!--more-->

## Hive使用

Hive 表面是使用 SQL 像在数据库中查询数据一样从 HDFS 查询数据.

本质是, Hive 把 SQL 转换成 MapReduce 程序去执行相应的操作, 并返回结果。

## 数据倾斜问题

### 表现

数据倾斜主要表现在，mapreduce 程序执行时，reduce 节点大部分执行完毕，但是有一个或者几个 reduce 节点运行很慢，导致整个程序的处理时间很长，这是因为某一个 key 的条数比其他 key 多很多 (有时是百倍或者千倍之多)，这条 Key 所在的 reduce 节点所处理的数据量比其他节点就大很多，从而导致某几 个节点迟迟运行不完。

- 任务进度长时间维持在 99%（或 100%），查看任务监控页面，发现只有少量（1 个或几个）reduce 子任务未完成。因为其处理的数据量和其他 reduce 差异过大。

- 单一 reduce 的记录数与平均记录数差异过大，通常可能达到 3 倍甚至更多。最长时长远大于平均时长。

  #### 引起这些表现的操作

- Join 其中一个表较小，但是 key 集中分发到某一个或几个 Reduce 上的数据远高于平均值。
- 分桶的判断字段 0 值或空值过多这些空值都由一个 reduce 处理，非常慢。
- group by  维度过小，某值的数量过多处理某值的 reduce 非常耗时。
- Count Distinct 某特殊值过多，处理此特殊值的 reduce 耗时。

### 处理

#### **1.MapJoin**

如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理。使用 map join 让小的维度表（1000 条以下的记录条数）先进内存。

#### 2.group by **在 map 端完成 reduce。**

默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。

（1）是否在Map端进行聚合，默认为True

```sql
hive.map.aggr = true
```

（2）在Map端进行聚合操作的条目数目

```sql
hive.groupby.mapaggr.checkinterval = 100000
```

（3）有数据倾斜的时候进行负载均衡（默认是false）

```sql
hive.groupby.skewindata = true
```

有数据倾斜的时候进行负载均衡，**当选项设定为 true，生成的查询计划会有两个 MR Job。**第一个 MR Job 中，Map 的输出结果集合会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。

#### 3.小文件进行合并

在map执行前合并小文件，减少map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。

```sql
set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
```

#### 4. 复杂文件增加Map数 128M 

当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。

增加map的方法为：根据computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数。通过调整 max 可以起到调整 map 数的作用，减小 max 可以增加 map 数，增大 max 可以减少 map 数。需要提醒的是，直接调整 mapred.map.tasks 这个参数是没有效果的。

##### 1．执行查询

```sql
hive (default)> select count(*) from emp;
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
```

##### 2．设置最大切片值为100个字节

```sql
hive (default)> set mapreduce.input.fileinputformat.split.maxsize=100;
hive (default)> select count(*) from emp;
Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1
```

#### 5.合理设置Reduce数

##### 1．调整reduce个数方法一

（1）每个Reduce处理的数据量默认是256MB

```sql
hive.exec.reducers.bytes.per.reducer=256000000
```

（2）每个任务最大的reduce数，默认为1009

```sql
hive.exec.reducers.max=1009
```

（3）计算reducer数的公式

N=min(参数2，总输入数据量/参数1)

##### 2．调整reduce个数方法二

在hadoop的mapred-default.xml文件中修改

设置每个job的Reduce个数

```sql
set mapreduce.job.reduces = 15;
```

##### 3．reduce个数并不是越多越好

1）过多的启动和初始化reduce也会消耗时间和资源；

2）另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；

在设置reduce个数的时候也需要考虑这两个原则：处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适；

#### 4.其他

- 把空值的 key 变成一个字符串加上随机数，把倾斜的数据分到不同的 reduce 上，由于 null 值关联不上，处 理后并不影响最终结果。

- count distinct 大量相同特殊值：count distinct 时，将值为空的情况单独处理，如果是计算 count distinct，可以不用处理，直接过滤，在最后结果中加 1。如果还有其他计算，需要进行 group by，可以先将值为空的记录单独处理，再和其他计算结果进行 union。

## 自定义函数

### 三种自定义函数区别

- UDF：用户自定义函数，user defined function。单行进入，单行输出。UDF 操作作用于单个数据行，并且产生一个数据行作为输出。大多数函数都属于这一类（比如数学函数和字符串函数）。
- UDTF：用户自定义表生成函数。user defined table-generate function，单行输入，多行输出。UDTF 操作作用于单个数据行，并且产生多个数据行，一个表作为输出。
- UDAF：用户自定义聚合函数。user defined aggregate function，多行进入，单行输出。UDAF 接受多个输入数据行，并产生一个输出数据行。像 COUNT 和 MAX 这样的函数就是聚集函数。

### 定义过程

1. 继承 UDF 或者 UDAF 或者 UDTF，实现特定的方法；

   ```sql
     org.apache.hadoop.hive.ql.udf.generic.GenericUDF 
     org.apache.hadoop.hive.ql.udf.generic.GenericUDTF
   ```

2. 将写好的类打包为 jar，如 hivefirst.jar；

3. 进入到 Hive 外壳环境中，利用 add jar /home/hadoop/hivefirst.jar 注册该 jar 文件；

   ```sql
   add jar linux_jar_path
   ```

4. 为该类起一个别名，create temporary function mylength as 'com.whut.StringLength'，这里注意 UDF 只是为这个 Hive 会话临时定义的；

   ```sql
   create [temporary] function [dbname.]function_name AS class_name;
   ```

5. 在 select 中使用 该函数。

> 删除函数 drop [temporary] function [if exists] [dbname.]function_name;

### 示例

1．创建一个Maven工程Hive

2．导入依赖

```xml
<dependencies>        
    <!--  https://mvnrepository.com/artifact/org.apache.hive/hive-exec -->        <dependency>            
    <groupId>org.apache.hive</groupId>            
    <artifactId>hive-exec</artifactId>            
    <version>2.3.5</version>        
   </dependency>  
</dependencies>  
```

3．创建一个类

```java
package com.doit.hive;  
import org.apache.hadoop.hive.ql.exec.UDF;     
public class Lower extends UDF {       
    public String evaluate  (final String s) {            
        if (s == null) {            
            return  null;        
        }        
        return  s.toLowerCase();    
    } 
}  
```

4．打成jar包上传到服务器/opt/module/jars/udf.jar

5．将jar包添加到hive的classpath 

```sql
hive (default)> add jar /opt/module/datas/udf.jar;
```

6．创建临时函数与开发好的java class关联

```sql
hive (default)> create temporary function mf as "com._51doit.func.MyFunction";
```

7．即可在hql中使用自定义的函数strip 

```sql
hive (default)> select ename, mylower(ename) lowername from emp;
```

永久函数的使用:

把自定义函数的jar上传到hdfs中.

```bash
hdfs dfs -put lower.jar 'hdfs:///path/to/hive_func';
```

创建永久函数

```sql
hive> create function xxoo_lower as 'com._51doit.func.MyFunction'  using  jar 'hdfs:///path/to/hive_func/lower.jar'
```

 　3. 验证

```sql
hive> select xxoo_lower("Hello World");
hive> show functions;
--删除
hive> drop function xxoo_lower;
```

## Hive 的 cluster by、sort by、distribute by、orderby 区别?

- order by：全局排序，一个 reducer；
- sort by：分区内排序；
- distribute by：控制 map 结果的分发，相同值会被分发到同一个 map；
- cluster by：当 distribute by 和 sort by 用的同一个字段，可以用这个关键字，不能指定排序顺序。

## Hive 分区和分桶的区别

- 分区针对的是**数据的存储路径**；分桶针对的是**数据文件**。

- 分区可以提高查询效率，实际上 hive 的一个分区就是 HDFS 上的一个目录，目录里放着属于该分区的数据文件。

- **分区提高了数据的查询效率，同时还能将数据隔离开**，但是并非所有数据能形成合理的分区。hive 可以将数据进行分桶，不同于分区是针对存储路径进行分类，**分桶是在数据文件中对数据进行划分的一种技术**。分桶是指定某一列，让该列数据按照哈希取模的方式随机、均匀地分发到各个桶文件中。

## 动态分区和静态分区的区别及使用场景

### 分区的定义

分区表实际上就是对应一个HDFS文件系统上的独立的**文件夹**，该文件夹下是该分区所有的数据文件。Hive中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。

### 静态分区

对于静态分区，从字面就可以理解：表的分区数量和分区值是固定的。静态分区需要手动指定，列是在编译时期通过用户传递来决定的。

应用场景：需要提前知道所有分区。适用于分区定义得早且数量少的用例，不适用于生产。

### 动态分区

是基于查询参数的位置去推断分区的名称，只有在 SQL 执行时才能确定，会根据数据自动的创建新的分区。

示例

创建分区表

```sql
--源数据表
create table demo(
id  int ,
birthday string ,
cost int
)
row format delimited fields terminated by '\t' ;

create table demo2(
id int ,
cost int
) partitioned by(x string)
row format delimited fields terminated by '\t' ;
```

设置参数

```sql
set hive.exec.dynamic.partition=true //使用动态分区
set hive.exec.dynamic.partition.mode=nonstrick;//无限制模式，如果模式是strict，则必须有一个静态分区且放在最前面
set hive.exec.max.dynamic.partitions.pernode=10000;//每个节点生成动态分区的最大个数
set hive.exec.max.dynamic.partitions=100000;//生成动态分区的最大个数
set hive.exec.max.created.files=150000;//一个任务最多可以创建的文件数目
set dfs.datanode.max.xcievers=8192;//限定一次最多打开的文件数
set hive.merge.mapfiles=true; // map端的结果进行合并
set mapreduce.reduce.tasks =20000; //设置reduce task个数 增加reduce阶段的并行度
```

加载数据

```sql
insert into table demo2 partition(x) --此处的分区变量x应该跟demo2中的分区变量名一致
select id , cost ,birthday from demo; --select中的最后一个表达式birthday，会作为分区变量x的动态值，注意顺序
```

动态分区的底层机制： 

mapreduce中的多路输出mutipleOutputs(根据条件判断，将结果写入不同目录不同文件）

> 动态分区的意义： 不知道分区字段究竟有多少种类型时，使用静态分区不好实现；用动态分区，则可以自动完成数据的划分！

应用场景：有很多分区，无法提前预估新分区，动态分区是合适的，一般用于生产环境。

