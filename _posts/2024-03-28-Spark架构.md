---
layout: post
title: Spark架构
tags: [Spark]
categories: Demo
excerpt_separator: <!--more-->
---

spark
<!--more-->

## Spark

#### **Spark Core**

Spark Core 中提供了 Spark 最基础与最核心的功能，Spark 其他的功能如：Spark SQL，Spark Streaming，GraphX, MLlib 都是在 Spark Core 的基础上进行扩展的

#### **Spark** **SQL**

Spark SQL 是 Spark 用来操作结构化数据的组件。通过 Spark SQL，用户可以使用 SQL 或者 Apache Hive 版本的 SQL 方言（HQL）来查询数据。

#### **Spark Streaming**

Spark Streaming 是 Spark 平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的 API。

#### **Spark MLlib**

MLlib 是 Spark 提供的一个机器学习算法库。MLlib 不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语。

#### **Spark GraphX**

GraphX 是 Spark 面向图计算提供的框架与算法库。



## 一些重要角色：

- **Master** ：是一个Java进程，接收Worker的注册信息和心跳、移除异常超时的Worker、接收客户端提交的任务、负责资源调度、命令Worker启动Executor。

- **Worker** ：是一个Java进程，负责管理当前节点的资源管理，向Master注册并定期发送心跳，负责启动Executor、并监控Executor的状态。

- **SparkSubmit** ：是一个Java进程，负责向Master提交任务。

- **Driver ：**是很多类的统称，可以认为SparkContext就是Driver，client模式Driver运行在SparkSubmit进程中，cluster模式单独运行在一个进程中，负责将用户编写的代码转成Tasks，然后调度到Executor中执行，并监控Task的状态和执行进度。

- **Executor** ：是一个Java进程，负责执行Driver端生成的Task，将Task放入线程中运行。

  

## 一些重要概念

#### **Application**

使用SparkSubmit提交的个计算应用，一个Application中可以触发多次Action，触发一次Action产生一个Job，一个Application中可以有一到多个Job

#### **Job**

Driver向Executor提交的作业，触发一次Acition形成一个完整的DAG，一个DAG对应一个Job，一个Job中有一到多个Stage，一个Stage中有一到多个Task

#### **DAG**

概念：有向无环图，是对多个RDD转换过程和依赖关系的描述，触发Action就会形成一个完整的DAG，一个DAG对应一个Job

#### **Stage**

概念：任务执行阶段，Stage执行是有先后顺序的，先执行前的，在执行后面的，一个Stage对应一个TaskSet，一个TaskSet中的Task的数量取决于Stage中最后一个RDD分区的数量

#### **Task**

概念：Spark中任务最小的执行单元，Task分类两种，即ShuffleMapTask和ResultTask

Task其实就是类的实例，有属性（从哪里读取数据），有方法（如何计算），Task的数量决定并行度，同时也要考虑可用的cores  

#### **TaskSet**

保存同一种计算逻辑多个Task的集合，一个TaskSet中的Task计算逻辑都一样，计算的数据不一样

{% include aligner.html images="feature-img/spark01.png" column=1 %}


- Job：RDD每一个行动操作都会生成一个或者多个调度阶段 调度阶段（Stage）：每个Job都会根据依赖关系，以Shuffle过程作为划分，分为Shuffle Map Stage和Result Stage。每个Stage对应一个TaskSet，一个Task中包含多Task，TaskSet的数量与该阶段最后一个RDD的分区数相同。　
- Task：分发到Executor上的工作任务，是Spark的最小执行单元　 
- DAGScheduler：DAGScheduler是将DAG根据宽依赖将切分Stage，负责划分调度阶段并Stage转成TaskSet提交给TaskScheduler　
- TaskScheduler：TaskScheduler是将Task调度到Worker下的Exexcutor进程，然后丢入到Executor的线程池的中进行执行　



## **MapReduce和Spark的本质区别：**

1. MR只能做离线计算，如果实现复杂计算逻辑，一个MR搞不定，就需要将多个MR按照先后顺序连成一串，一个MR计算完成后会将计算结果写入到HDFS中，下一个MR将上一个MR的输出作为输入，这样就要频繁读写HDFS，网络IO和磁盘IO会成为性能瓶颈。从而导致效率低下。
2. spark既可以做离线计算，又可以做实时计算，提供了抽象的数据集（RDD、Dataset、DataFrame、DStream）

有高度封装的API，算子丰富，并且使用了更先进的DAG有向无环图调度思想，可以对执行计划优化后在执行，并且可以数据可以cache到内存中进行复用，shuffle时，数据可以不排序。

## Spark算子

### Spark 的 transformation 算子

#### map

映射，即将原来的RDD中**对应的**每一个元素，应用外部传入的函数进行运算，返回一个新的RDD

```scala
 val rdd1: RDD[Int] = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 2)
 val rdd2: RDD[Int] = rdd1.map(_ * 2)
// 2468101214161820
```

#### flatMap：

扁平化映射，即将原来RDD中对应的每一个元素应用外部的运算逻辑进行运算，然后再将返回的数据进行压平，类似先map，然后再flatten的操作，最后返回一个新的RDD

```scala
val arr = Array(
  "spark hive flink",
  "hive hive flink",
  "hive spark flink",
  "hive spark flink"
)
val rdd1: RDD[String] = sc.makeRDD(arr, 2)
val rdd2: RDD[String] = rdd1.flatMap(_.split(" "))
// sparkhiveflinkhivehiveflinkhivesparkflinkhivesparkflink
```

#### filter：

过滤，即将原来RDD中对应的每一个元素，应用外部传入的过滤逻辑，然后返回一个新的的RDD

```scala
 val rdd1: RDD[Int] = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 2)
 val rdd2: RDD[Int] = rdd1.filter(_ % 2 == 0)
```

#### mapPartitions：

将数据以**分区**为的形式返回进行map操作，一个分区对应一个迭代器，该方法和map方法类似，只不过该方法的参数由RDD中的每一个元素变成了RDD中每一个分区的迭代器，如果在映射的过程中需要频繁创建额外的对象，使用mapPartitions要比map高效的过。

```scala
val rdd1 = sc.parallelize(List(1, 2, 3, 4, 5), 2)
var r1: RDD[Int] = rdd1.mapPartitions(it => it.map(x => x * 10))
```

注意：map 与 mapPartition 区别

（1）map：每次处理一条数据

（2）mapPartitions：每次处理一个分区数据

**mapPartitions一定会比map效率更高吗？**

不一定：如果对RDD中的数据进行简单的映射操作，例如变大写，对数据进行简单的运算，map和mapPartitions的效果是一样的，但是如果是使用到了外部共享的对象或数据库连接，mapPartitions效率会更高一些。

#### reduceByKey：

将数据按照相同的key进行聚合，特点是先在每个分区中进行局部分组聚合，然后将每个分区聚合的结果从上游拉取到下游再进行全局分组聚合

```scala
val lst = List(
  ("spark", 1), ("hadoop", 1), ("hive", 1), ("spark", 1),
  ("spark", 1), ("flink", 1), ("hbase", 1), ("spark", 1),
  ("kafka", 1), ("kafka", 1), ("kafka", 1), ("kafka", 1),
  ("hadoop", 1), ("flink", 1), ("hive", 1), ("flink", 1)
)
//通过并行化的方式创建RDD，分区数量为4
val wordAndOne: RDD[(String, Int)] = sc.parallelize(lst, 4)
val reduced: RDD[(String, Int)] = wordAndOne.reduceByKey(_ + _)
// (hive,2)(flink,3)(spark,4)(hadoop,2)(hbase,1)(kafka,4)
```

#### groupByKey ：

按照key进行分组(仅分组不聚合)，底层使用的是ShuffledRDD，mapSideCombine = false，传入的三个函数只有前两个被调用了，并且是在下游执行的

```scala
val lst = List(
  ("spark", 1), ("hadoop", 1), ("hive", 1), ("spark", 1),
  ("spark", 1), ("flink", 1), ("hbase", 1), ("spark", 1),
  ("kafka", 1), ("kafka", 1), ("kafka", 1), ("kafka", 1),
  ("hadoop", 1), ("flink", 1), ("hive", 1), ("flink", 1)
)
//通过并行化的方式创建RDD，分区数量为4
val wordAndOne: RDD[(String, Int)] = sc.parallelize(lst, 4)
//按照key进行分组
val grouped: RDD[(String, Iterable[Int])] = wordAndOne.groupByKey()
// (hive,CompactBuffer(1, 1))(flink,CompactBuffer(1, 1, 1))(spark,CompactBuffer(1, 1, 1, 1))(hadoop,CompactBuffer(1, 1))(hbase,CompactBuffer(1))(kafka,CompactBuffer(1, 1, 1, 1))

```

**reduceByKey 与 groupByKey 的区别**

reduceByKey：具有预聚合操作

groupByKey：没有预聚合

**在不影响业务逻辑的前提下，优先采用 reduceByKey。**

*从 shuffle 的角度*：reduceByKey 和 groupByKey 都存在 shuffle 的操作，但是 reduceByKey 可以在 shuffle 前对分区内相同 key 的数据进行预聚合（combine）功能，这样会减少落盘的数据量，而 groupByKey 只是进行分组，不存在数据量减少的问题，reduceByKey 性能比较高。

*从功能的角度*：reduceByKey 其实包含分组和聚合的功能。groupByKey 只能分组，不能聚合，所以在分组聚合的场合下，推荐使用 reduceByKey，如果仅仅是分组而不需要聚合。那么还是只能使用 groupByKey

#### foldByKey：

与reduceByKey类似，只不过是可以指定初始值，每个分区应用一次初始值，先在每个进行局部聚合，然后再全局聚合，局部聚合的逻辑与全局聚合的逻辑相同。

```scala
val lst: Seq[(String, Int)] = List(
  ("spark", 1), ("hadoop", 1), ("hive", 1), ("spark", 1),
  ("spark", 1), ("flink", 1), ("hbase", 1), ("spark", 1),
  ("kafka", 1), ("kafka", 1), ("kafka", 1), ("kafka", 1),
  ("hadoop", 1), ("flink", 1), ("hive", 1), ("flink", 1)
)
//通过并行化的方式创建RDD，分区数量为4
val wordAndOne: RDD[(String, Int)] = sc.parallelize(lst, 4)

//与reduceByKey类似，只不过是可以指定初始值，每个分区应用一次初始值
val reduced: RDD[(String, Int)] = wordAndOne.foldByKey(0)(_ + _)
// (hive,2)(flink,3)(spark,4)(hadoop,2)(hbase,1)(kafka,4)
val reduced: RDD[(String, Int)] = wordAndOne.foldByKey(10)(_ + _)
// (hive,22)(flink,23)(spark,24)(hadoop,22)(hbase,11)(kafka,14)
```

#### aggregateByKey：

与reduceByKey类似，并且可以指定初始值，每个分区应用一次初始值，传入两个函数，分别是局部聚合的计算逻辑、全局聚合的逻辑。

```scala
val lst: Seq[(String, Int)] = List(
  ("spark", 1), ("hadoop", 1), ("hive", 1), ("spark", 1),
  ("spark", 1), ("flink", 1), ("hbase", 1), ("spark", 1),
  ("kafka", 1), ("kafka", 1), ("kafka", 1), ("kafka", 1),
  ("hadoop", 1), ("flink", 1), ("hive", 1), ("flink", 1)
)
//通过并行化的方式创建RDD，分区数量为4
val wordAndOne: RDD[(String, Int)] = sc.parallelize(lst, 4)
//在第一个括号中传入初始化，第二个括号中传入两个函数，分别是局部聚合的逻辑和全局聚合的逻辑
val reduced: RDD[(String, Int)] = wordAndOne.aggregateByKey(0)(_ + _, _ + _)
// (hive,2)(flink,3)(spark,4)(hadoop,2)(hbase,1)(kafka,4)
val reduced1: RDD[(String, Int)] = wordAndOne.aggregateByKey(10)(_ + _, _ + _)
// (hive,22)(flink,23)(spark,24)(hadoop,22)(hbase,11)(kafka,14)

```

#### combineByKey

要传入三个函数(mapSideCombine = true)：

第一个函数：在上游执行，该key在当前分区第一次出现时，对value处理的运算逻辑

第二个函数：在上游执行，当该key在当前分区再次出现时，将以前相同key的value进行运算的逻辑

第三个函数：在下游执行，将来自不同分区，相同key的数据通过网络拉取过来，然后进行全局聚合的逻辑

```scala
val lst = List(
  ("spark", 1), ("hadoop", 1), ("hive", 1), ("spark", 1),
  ("spark", 1), ("flink", 1), ("hbase", 1), ("spark", 1),
  ("kafka", 1), ("kafka", 1), ("kafka", 1), ("kafka", 1),
  ("hadoop", 1), ("flink", 1), ("hive", 1), ("flink", 1)
)
//通过并行化的方式创建RDD，分区数量为4
val wordAndOne: RDD[(String, Int)] = sc.parallelize(lst, 4)
//调用combineByKey传入三个函数
//val reduced = wordAndOne.combineByKey(x => x, (a: Int, b: Int) => a + b, (m: Int, n: Int) => m + n)
val f1 = (x: Int) => {
  val stage = TaskContext.get().stageId()
  val partition = TaskContext.getPartitionId()
  println(s"f1 function invoked in state: $stage, partition: $partition")
  x
}
//在每个分区内，将key相同的value进行局部聚合操作
val f2 = (a: Int, b: Int) => {
  val stage = TaskContext.get().stageId()
  val partition = TaskContext.getPartitionId()
  println(s"f2 function invoked in state: $stage, partition: $partition")
  a + b
}
//第三个函数是在下游完成的
val f3 = (m: Int, n: Int) => {
  val stage = TaskContext.get().stageId()
  val partition = TaskContext.getPartitionId()
  println(s"f3 function invoked in state: $stage, partition: $partition")
  m + n
}
val reduced = wordAndOne.combineByKey(f1, f2, f3)
//f1 function invoked in state: 0, partition: 1
//f1 function invoked in state: 0, partition: 0
//f1 function invoked in state: 0, partition: 3
//f1 function invoked in state: 0, partition: 2
//f1 function invoked in state: 0, partition: 0
//f1 function invoked in state: 0, partition: 3
//f1 function invoked in state: 0, partition: 1
//f2 function invoked in state: 0, partition: 2
//f2 function invoked in state: 0, partition: 2
//f1 function invoked in state: 0, partition: 0
//f1 function invoked in state: 0, partition: 3
//f1 function invoked in state: 0, partition: 1
//f2 function invoked in state: 0, partition: 2
//f2 function invoked in state: 0, partition: 0
//f2 function invoked in state: 0, partition: 3
//f2 function invoked in state: 0, partition: 1
//f3 function invoked in state: 1, partition: 1
//f3 function invoked in state: 1, partition: 0
//f3 function invoked in state: 1, partition: 0
//f3 function invoked in state: 1, partition: 1
//(hive,2)(flink,3)(spark,4)(hadoop,2)(hbase,1)(kafka,4)
```



### reduceByKey、foldByKey、aggregateByKey、combineByKey 区别

|      算子      |                   区别                    |
| :------------: | :---------------------------------------: |
|  ReduceByKey   |     没有初始值 分区内和分区间逻辑相同     |
|   foldByKey    |      有初始值 分区内和分区间逻辑相同      |
| aggregateByKey |    有初始值 分区内和分区间逻辑可以不同    |
|  combineByKey  | 初始值可以变化结构 分区内和分区间逻辑不同 |

#### sortBy

按照指的的排序规则进行全局排序，且sortBy函数函数的实现依赖于sortByKey函数

**第一个参数**是一个函数，该函数的也有一个带T泛型的参数，返回类型和RDD中元素的类型是一致的；

**第二个参数**是ascending，从字面的意思大家应该可以猜到，是的，这参数决定排序后RDD中的元素是升序还是降序，默认是true，也就是升序；

**第三个参数**是numPartitions，该参数决定排序后的RDD的分区个数，默认排序后的分区个数和排序之前的个数相等，即为this.partitions.size。

```scala
  val lines: RDD[String] = sc.textFile("data/words.txt")
  //切分压平
  val words: RDD[String] = lines.flatMap(_.split(" "))
  //将单词和1组合
  val wordAndOne: RDD[(String, Int)] = words.map((_, 1))
  //分组聚合
  val reduced: RDD[(String, Int)] = wordAndOne.reduceByKey(_ + _)
  //按照单词出现的次数，从高到低进行排序
  val sorted: RDD[(String, Int)] = reduced.sortBy(_._2, false)
  sorted.collect().foreach(println)
  
//  (your, 5)
//  (who, 3)
//  (to, 3)
//  (the, 3)
//  (someone, 2)
//  (when, 2)
//  (stand, 2)
//  (people, 2)
//  (you, 2)
//  (that, 2)
//  (at, 2)
//  (you
//  ’re
//  , 2
//  )
//  (you., 2)
//  (for, 2)
//  (beside, 1)
//  (are, 1)
//  (shouldn
//  ’t
//  , 1
//  )
//  (Never,, 1)
//  (is, 1)
//  (best,, 1)
//  (spot., 1)
//  (short, 1)
//  (worth., 1)
//  (have, 1)
//  (suck, 1)
//  (their, 1)
//  (
//  with, 1
//  )
//  (spend, 1)
//  (life,, 1)
//  (ever, 1)
//  (continuously, 1)
//  (make, 1)
//  (true, 1)
//  (Life, 1)
//  (out, 1)
//  (too, 1)
//  (insist, 1)
//  (overlooks, 1)
//  (fight, 1)
//  (happiness, 1)
//  (And, 1)
//  (remember,, 1)
//  (not, 1)
//  (friends., 1)
//  (a, 1)
//  (You, 1)
//  (they’ll, 1)
//  (it’s, 1)
//  (room, 1)
//  (yourself, 1)
//  (in, 1)
//  (ones, 1)
//  (of, 1)
//  (side, 1)
//  (by, 1)
//  (If, 1)
//  (but, 1)
//  (time, 1)
//  (worst, 1)
//  (wants, 1)
```

#### sortByKey

函数作用于Key-Value形式的RDD，并对Key进行排序。该函数返回的RDD一定是ShuffledRDD类型的，因为对源RDD进行排序，必须进行Shuffle操作，而Shuffle操作的结果RDD就是ShuffledRDD。其实这个函数的实现很优雅，里面用到了RangePartitioner，在构建RangePartitioner时，会对数据进行采样，所有会触发Action，根据采样的结果来构建RangePartitioner，所以会生成job。RangePartitioner可以使得相应的范围Key数据分到同一个partition中，然后内部用到了mapPartitions对每个partition中的数据进行排序，而每个partition中数据的排序用到了标准的sort机制，避免了大量数据的shuffle。



### Spark 的 action算子

（1）reduce（2）collect（3）count（4）first（5）take（6）takeOrdered（7）aggregate（8）fold

（9）countByKey（10）save（11）foreach