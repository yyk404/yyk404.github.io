---
layout: post
title: Spark架构
tags: [Spark]
categories: Demo
excerpt_separator: <!--more-->
---

spark
<!--more-->

## Spark

#### **Spark Core**

Spark Core 中提供了 Spark 最基础与最核心的功能，Spark 其他的功能如：Spark SQL，Spark Streaming，GraphX, MLlib 都是在 Spark Core 的基础上进行扩展的

#### **Spark** **SQL**

Spark SQL 是 Spark 用来操作结构化数据的组件。通过 Spark SQL，用户可以使用 SQL 或者 Apache Hive 版本的 SQL 方言（HQL）来查询数据。

#### **Spark Streaming**

Spark Streaming 是 Spark 平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的 API。

#### **Spark MLlib**

MLlib 是 Spark 提供的一个机器学习算法库。MLlib 不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语。

#### **Spark GraphX**

GraphX 是 Spark 面向图计算提供的框架与算法库。



## 一些重要角色：

- **Master** ：是一个Java进程，接收Worker的注册信息和心跳、移除异常超时的Worker、接收客户端提交的任务、负责资源调度、命令Worker启动Executor。

- **Worker** ：是一个Java进程，负责管理当前节点的资源管理，向Master注册并定期发送心跳，负责启动Executor、并监控Executor的状态。

- **SparkSubmit** ：是一个Java进程，负责向Master提交任务。

- **Driver ：**是很多类的统称，可以认为SparkContext就是Driver，client模式Driver运行在SparkSubmit进程中，cluster模式单独运行在一个进程中，负责将用户编写的代码转成Tasks，然后调度到Executor中执行，并监控Task的状态和执行进度。

- **Executor** ：是一个Java进程，负责执行Driver端生成的Task，将Task放入线程中运行。

  

## 一些重要概念

#### **Application**

使用SparkSubmit提交的个计算应用，一个Application中可以触发多次Action，触发一次Action产生一个Job，一个Application中可以有一到多个Job

#### **Job**

Driver向Executor提交的作业，触发一次Acition形成一个完整的DAG，一个DAG对应一个Job，一个Job中有一到多个Stage，一个Stage中有一到多个Task

#### **DAG**

概念：有向无环图，是对多个RDD转换过程和依赖关系的描述，触发Action就会形成一个完整的DAG，一个DAG对应一个Job

#### **Stage**

概念：任务执行阶段，Stage执行是有先后顺序的，先执行前的，在执行后面的，一个Stage对应一个TaskSet，一个TaskSet中的Task的数量取决于Stage中最后一个RDD分区的数量

#### **Task**

概念：Spark中任务最小的执行单元，Task分类两种，即ShuffleMapTask和ResultTask

Task其实就是类的实例，有属性（从哪里读取数据），有方法（如何计算），Task的数量决定并行度，同时也要考虑可用的cores  

#### **TaskSet**

保存同一种计算逻辑多个Task的集合，一个TaskSet中的Task计算逻辑都一样，计算的数据不一样

{% include aligner.html images="feature-img/spark01.png" column=1 %}


- Job：RDD每一个行动操作都会生成一个或者多个调度阶段 调度阶段（Stage）：每个Job都会根据依赖关系，以Shuffle过程作为划分，分为Shuffle Map Stage和Result Stage。每个Stage对应一个TaskSet，一个Task中包含多Task，TaskSet的数量与该阶段最后一个RDD的分区数相同。　
- Task：分发到Executor上的工作任务，是Spark的最小执行单元　 
- DAGScheduler：DAGScheduler是将DAG根据宽依赖将切分Stage，负责划分调度阶段并Stage转成TaskSet提交给TaskScheduler　
- TaskScheduler：TaskScheduler是将Task调度到Worker下的Exexcutor进程，然后丢入到Executor的线程池的中进行执行　



## **MapReduce和Spark的本质区别：**

1. MR只能做离线计算，如果实现复杂计算逻辑，一个MR搞不定，就需要将多个MR按照先后顺序连成一串，一个MR计算完成后会将计算结果写入到HDFS中，下一个MR将上一个MR的输出作为输入，这样就要频繁读写HDFS，网络IO和磁盘IO会成为性能瓶颈。从而导致效率低下。
2. spark既可以做离线计算，又可以做实时计算，提供了抽象的数据集（RDD、Dataset、DataFrame、DStream）

有高度封装的API，算子丰富，并且使用了更先进的DAG有向无环图调度思想，可以对执行计划优化后在执行，并且可以数据可以cache到内存中进行复用，shuffle时，数据可以不排序。

## Spark算子

### Spark 的 transformation 算子

#### map

映射，即将原来的RDD中**对应的**每一个元素，应用外部传入的函数进行运算，返回一个新的RDD

```scala
 val rdd1: RDD[Int] = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 2)
 val rdd2: RDD[Int] = rdd1.map(_ * 2)
// 2468101214161820
```

#### flatMap：

扁平化映射，即将原来RDD中对应的每一个元素应用外部的运算逻辑进行运算，然后再将返回的数据进行压平，类似先map，然后再flatten的操作，最后返回一个新的RDD

```scala
val arr = Array(
  "spark hive flink",
  "hive hive flink",
  "hive spark flink",
  "hive spark flink"
)
val rdd1: RDD[String] = sc.makeRDD(arr, 2)
val rdd2: RDD[String] = rdd1.flatMap(_.split(" "))
// sparkhiveflinkhivehiveflinkhivesparkflinkhivesparkflink
```

#### filter：

过滤，即将原来RDD中对应的每一个元素，应用外部传入的过滤逻辑，然后返回一个新的的RDD

```scala
 val rdd1: RDD[Int] = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 2)
 val rdd2: RDD[Int] = rdd1.filter(_ % 2 == 0)
```

#### mapPartitions：

将数据以**分区**为的形式返回进行map操作，一个分区对应一个迭代器，该方法和map方法类似，只不过该方法的参数由RDD中的每一个元素变成了RDD中每一个分区的迭代器，如果在映射的过程中需要频繁创建额外的对象，使用mapPartitions要比map高效的过。

```scala
val rdd1 = sc.parallelize(List(1, 2, 3, 4, 5), 2)
var r1: RDD[Int] = rdd1.mapPartitions(it => it.map(x => x * 10))
```

注意：map 与 mapPartition 区别

（1）map：每次处理一条数据

（2）mapPartitions：每次处理一个分区数据

**mapPartitions一定会比map效率更高吗？**

不一定：如果对RDD中的数据进行简单的映射操作，例如变大写，对数据进行简单的运算，map和mapPartitions的效果是一样的，但是如果是使用到了外部共享的对象或数据库连接，mapPartitions效率会更高一些。

#### reduceByKey：

将数据按照相同的key进行聚合，特点是先在每个分区中进行局部分组聚合，然后将每个分区聚合的结果从上游拉取到下游再进行全局分组聚合

```scala
val lst = List(
  ("spark", 1), ("hadoop", 1), ("hive", 1), ("spark", 1),
  ("spark", 1), ("flink", 1), ("hbase", 1), ("spark", 1),
  ("kafka", 1), ("kafka", 1), ("kafka", 1), ("kafka", 1),
  ("hadoop", 1), ("flink", 1), ("hive", 1), ("flink", 1)
)
//通过并行化的方式创建RDD，分区数量为4
val wordAndOne: RDD[(String, Int)] = sc.parallelize(lst, 4)
val reduced: RDD[(String, Int)] = wordAndOne.reduceByKey(_ + _)
// (hive,2)(flink,3)(spark,4)(hadoop,2)(hbase,1)(kafka,4)
```

#### groupByKey ：

按照key进行分组(仅分组不聚合)，底层使用的是ShuffledRDD，mapSideCombine = false，传入的三个函数只有前两个被调用了，并且是在下游执行的

```scala
val lst = List(
  ("spark", 1), ("hadoop", 1), ("hive", 1), ("spark", 1),
  ("spark", 1), ("flink", 1), ("hbase", 1), ("spark", 1),
  ("kafka", 1), ("kafka", 1), ("kafka", 1), ("kafka", 1),
  ("hadoop", 1), ("flink", 1), ("hive", 1), ("flink", 1)
)
//通过并行化的方式创建RDD，分区数量为4
val wordAndOne: RDD[(String, Int)] = sc.parallelize(lst, 4)
//按照key进行分组
val grouped: RDD[(String, Iterable[Int])] = wordAndOne.groupByKey()
// (hive,CompactBuffer(1, 1))(flink,CompactBuffer(1, 1, 1))(spark,CompactBuffer(1, 1, 1, 1))(hadoop,CompactBuffer(1, 1))(hbase,CompactBuffer(1))(kafka,CompactBuffer(1, 1, 1, 1))

```

**reduceByKey 与 groupByKey 的区别**

reduceByKey：具有预聚合操作

groupByKey：没有预聚合

**在不影响业务逻辑的前提下，优先采用 reduceByKey。**

*从 shuffle 的角度*：reduceByKey 和 groupByKey 都存在 shuffle 的操作，但是 reduceByKey 可以在 shuffle 前对分区内相同 key 的数据进行预聚合（combine）功能，这样会减少落盘的数据量，而 groupByKey 只是进行分组，不存在数据量减少的问题，reduceByKey 性能比较高。

*从功能的角度*：reduceByKey 其实包含分组和聚合的功能。groupByKey 只能分组，不能聚合，所以在分组聚合的场合下，推荐使用 reduceByKey，如果仅仅是分组而不需要聚合。那么还是只能使用 groupByKey

#### foldByKey：

与reduceByKey类似，只不过是可以指定初始值，每个分区应用一次初始值，先在每个进行局部聚合，然后再全局聚合，局部聚合的逻辑与全局聚合的逻辑相同。

```scala
val lst: Seq[(String, Int)] = List(
  ("spark", 1), ("hadoop", 1), ("hive", 1), ("spark", 1),
  ("spark", 1), ("flink", 1), ("hbase", 1), ("spark", 1),
  ("kafka", 1), ("kafka", 1), ("kafka", 1), ("kafka", 1),
  ("hadoop", 1), ("flink", 1), ("hive", 1), ("flink", 1)
)
//通过并行化的方式创建RDD，分区数量为4
val wordAndOne: RDD[(String, Int)] = sc.parallelize(lst, 4)

//与reduceByKey类似，只不过是可以指定初始值，每个分区应用一次初始值
val reduced: RDD[(String, Int)] = wordAndOne.foldByKey(0)(_ + _)
// (hive,2)(flink,3)(spark,4)(hadoop,2)(hbase,1)(kafka,4)
val reduced: RDD[(String, Int)] = wordAndOne.foldByKey(10)(_ + _)
// (hive,22)(flink,23)(spark,24)(hadoop,22)(hbase,11)(kafka,14)
```

#### aggregateByKey：

与reduceByKey类似，并且可以指定初始值，每个分区应用一次初始值，传入两个函数，分别是局部聚合的计算逻辑、全局聚合的逻辑。

```scala
val lst: Seq[(String, Int)] = List(
  ("spark", 1), ("hadoop", 1), ("hive", 1), ("spark", 1),
  ("spark", 1), ("flink", 1), ("hbase", 1), ("spark", 1),
  ("kafka", 1), ("kafka", 1), ("kafka", 1), ("kafka", 1),
  ("hadoop", 1), ("flink", 1), ("hive", 1), ("flink", 1)
)
//通过并行化的方式创建RDD，分区数量为4
val wordAndOne: RDD[(String, Int)] = sc.parallelize(lst, 4)
//在第一个括号中传入初始化，第二个括号中传入两个函数，分别是局部聚合的逻辑和全局聚合的逻辑
val reduced: RDD[(String, Int)] = wordAndOne.aggregateByKey(0)(_ + _, _ + _)
// (hive,2)(flink,3)(spark,4)(hadoop,2)(hbase,1)(kafka,4)
val reduced1: RDD[(String, Int)] = wordAndOne.aggregateByKey(10)(_ + _, _ + _)
// (hive,22)(flink,23)(spark,24)(hadoop,22)(hbase,11)(kafka,14)

```

#### combineByKey

要传入三个函数(mapSideCombine = true)：

第一个函数：在上游执行，该key在当前分区第一次出现时，对value处理的运算逻辑

第二个函数：在上游执行，当该key在当前分区再次出现时，将以前相同key的value进行运算的逻辑

第三个函数：在下游执行，将来自不同分区，相同key的数据通过网络拉取过来，然后进行全局聚合的逻辑

```scala
val lst = List(
  ("spark", 1), ("hadoop", 1), ("hive", 1), ("spark", 1),
  ("spark", 1), ("flink", 1), ("hbase", 1), ("spark", 1),
  ("kafka", 1), ("kafka", 1), ("kafka", 1), ("kafka", 1),
  ("hadoop", 1), ("flink", 1), ("hive", 1), ("flink", 1)
)
//通过并行化的方式创建RDD，分区数量为4
val wordAndOne: RDD[(String, Int)] = sc.parallelize(lst, 4)
//调用combineByKey传入三个函数
//val reduced = wordAndOne.combineByKey(x => x, (a: Int, b: Int) => a + b, (m: Int, n: Int) => m + n)
val f1 = (x: Int) => {
  val stage = TaskContext.get().stageId()
  val partition = TaskContext.getPartitionId()
  println(s"f1 function invoked in state: $stage, partition: $partition")
  x
}
//在每个分区内，将key相同的value进行局部聚合操作
val f2 = (a: Int, b: Int) => {
  val stage = TaskContext.get().stageId()
  val partition = TaskContext.getPartitionId()
  println(s"f2 function invoked in state: $stage, partition: $partition")
  a + b
}
//第三个函数是在下游完成的
val f3 = (m: Int, n: Int) => {
  val stage = TaskContext.get().stageId()
  val partition = TaskContext.getPartitionId()
  println(s"f3 function invoked in state: $stage, partition: $partition")
  m + n
}
val reduced = wordAndOne.combineByKey(f1, f2, f3)
//f1 function invoked in state: 0, partition: 1
//f1 function invoked in state: 0, partition: 0
//f1 function invoked in state: 0, partition: 3
//f1 function invoked in state: 0, partition: 2
//f1 function invoked in state: 0, partition: 0
//f1 function invoked in state: 0, partition: 3
//f1 function invoked in state: 0, partition: 1
//f2 function invoked in state: 0, partition: 2
//f2 function invoked in state: 0, partition: 2
//f1 function invoked in state: 0, partition: 0
//f1 function invoked in state: 0, partition: 3
//f1 function invoked in state: 0, partition: 1
//f2 function invoked in state: 0, partition: 2
//f2 function invoked in state: 0, partition: 0
//f2 function invoked in state: 0, partition: 3
//f2 function invoked in state: 0, partition: 1
//f3 function invoked in state: 1, partition: 1
//f3 function invoked in state: 1, partition: 0
//f3 function invoked in state: 1, partition: 0
//f3 function invoked in state: 1, partition: 1
//(hive,2)(flink,3)(spark,4)(hadoop,2)(hbase,1)(kafka,4)
```



### reduceByKey、foldByKey、aggregateByKey、combineByKey 区别

|      算子      |                   区别                    |
| :------------: | :---------------------------------------: |
|  ReduceByKey   |     没有初始值 分区内和分区间逻辑相同     |
|   foldByKey    |      有初始值 分区内和分区间逻辑相同      |
| aggregateByKey |    有初始值 分区内和分区间逻辑可以不同    |
|  combineByKey  | 初始值可以变化结构 分区内和分区间逻辑不同 |

#### sortBy

按照指的的排序规则进行全局排序，且sortBy函数函数的实现依赖于sortByKey函数

**第一个参数**是一个函数，该函数的也有一个带T泛型的参数，返回类型和RDD中元素的类型是一致的；

**第二个参数**是ascending，从字面的意思大家应该可以猜到，是的，这参数决定排序后RDD中的元素是升序还是降序，默认是true，也就是升序；

**第三个参数**是numPartitions，该参数决定排序后的RDD的分区个数，默认排序后的分区个数和排序之前的个数相等，即为this.partitions.size。

```scala
  val lines: RDD[String] = sc.textFile("data/words.txt")
  //切分压平
  val words: RDD[String] = lines.flatMap(_.split(" "))
  //将单词和1组合
  val wordAndOne: RDD[(String, Int)] = words.map((_, 1))
  //分组聚合
  val reduced: RDD[(String, Int)] = wordAndOne.reduceByKey(_ + _)
  //按照单词出现的次数，从高到低进行排序
  val sorted: RDD[(String, Int)] = reduced.sortBy(_._2, false)
  sorted.collect().foreach(println)
  
//  (your, 5)
//  (who, 3)
//  (to, 3)
//  (the, 3)
//  (someone, 2)
//  (when, 2)
//  (stand, 2)
//  (people, 2)
//  (you, 2)
//  (that, 2)
//  (at, 2)
//  (you
//  ’re
//  , 2
//  )
//  (you., 2)
//  (for, 2)
//  (beside, 1)
//  (are, 1)
//  (shouldn
//  ’t
//  , 1
//  )
//  (Never,, 1)
//  (is, 1)
//  (best,, 1)
//  (spot., 1)
//  (short, 1)
//  (worth., 1)
//  (have, 1)
//  (suck, 1)
//  (their, 1)
//  (
//  with, 1
//  )
//  (spend, 1)
//  (life,, 1)
//  (ever, 1)
//  (continuously, 1)
//  (make, 1)
//  (true, 1)
//  (Life, 1)
//  (out, 1)
//  (too, 1)
//  (insist, 1)
//  (overlooks, 1)
//  (fight, 1)
//  (happiness, 1)
//  (And, 1)
//  (remember,, 1)
//  (not, 1)
//  (friends., 1)
//  (a, 1)
//  (You, 1)
//  (they’ll, 1)
//  (it’s, 1)
//  (room, 1)
//  (yourself, 1)
//  (in, 1)
//  (ones, 1)
//  (of, 1)
//  (side, 1)
//  (by, 1)
//  (If, 1)
//  (but, 1)
//  (time, 1)
//  (worst, 1)
//  (wants, 1)
```

#### sortByKey

函数作用于Key-Value形式的RDD，并对Key进行排序。该函数返回的RDD一定是ShuffledRDD类型的，因为对源RDD进行排序，必须进行Shuffle操作，而Shuffle操作的结果RDD就是ShuffledRDD。其实这个函数的实现很优雅，里面用到了RangePartitioner，在构建RangePartitioner时，会对数据进行采样，所有会触发Action，根据采样的结果来构建RangePartitioner，所以会生成job。RangePartitioner可以使得相应的范围Key数据分到同一个partition中，然后内部用到了mapPartitions对每个partition中的数据进行排序，而每个partition中数据的排序用到了标准的sort机制，避免了大量数据的shuffle。

#### reparation

重新分区，一定会shuffle，即将数据随机打散。reparation的功能是改变分区数量（可以增大、减少、不变）可以将数据相对均匀的重新分区，可以改善数据倾斜的问题

```scala
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 3)
//repartition方法一定shuffle
//不论将分区数量变多、变少、或不变，都shuffle
val rdd2 = rdd1.repartition(3)
```

#### coalesce

可以shuffle，也可以不shuffle，如果将分区数量减少，并且shuffle = false，就是将分区进行合并

```scala
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 3)
//shuffle = true
val rdd2 = rdd1.coalesce(3, true)
//与repartition(3)功能一样
```

```scala
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 4)
//shuffle = false
val rdd2 = rdd1.coalesce(2, false)
```

**关系：**两者都是用来改变 RDD 的 partition 数量的，repartition 底层调用的就是 coalesce 方法：coalesce(numPartitions, shuffle = true)

**区别：**coalesce() 方法的参数 shuffle 默认设置为 false，coalesce 根据传入的参数来判断是否发生 shuffle。repartition() 方法就是 coalesce() 方法 shuffle 为 true 的情况，repartition 一定会发生 shuffle。

> 一般情况下增大 rdd 的 partition 数量使用 repartition，减少 partition 数量时使用 coalesce。

#### cogroup

协同分组，即将多个RDD中对应的数据，使用相同的分区器（HashPartitioner），将来自多个RDD中的key相同的数据通过网络传入到同一台机器的同一个分区中(与*groupByKey、groupBy区别是：groupByKey、groupBy只能对一个RDD进行分组*)

> 调用cogroup方法，两个RDD中对应的数据都必须是对偶元组类型，并且key类型一定相同

```scala
//通过并行化的方式创建一个RDD
val rdd1 = sc.parallelize(List(("tom", 1), ("tom", 2), ("jerry", 3), ("kitty", 2), ("jerry", 4)), 3)
//通过并行化的方式再创建一个RDD
val rdd2 = sc.parallelize(List(("jerry", 2), ("tom", 1), ("shuke", 2), ("jerry", 4)), 2)
//将两个RDD都进行分组
val grouped: RDD[(String, (Iterable[Int], Iterable[Int]))] = rdd1.cogroup(rdd2)
// (tom,(CompactBuffer(1, 2),CompactBuffer(1)))
// (jerry,(CompactBuffer(3, 4),CompactBuffer(2, 4)))
// (shuke,(CompactBuffer(),CompactBuffer(2)))
// (kitty,(CompactBuffer(2),CompactBuffer()))
```





### Spark 的 action算子

（1）reduce（2）collect（3）count（4）first（5）take（6）takeOrdered（7）aggregate（8）fold

（9）countByKey（10）save（11）foreach

> Action算子会触发Job的生成，底层调用的是sparkContext.runJob方法，根据最后一个RDD，**从后往前**，切分Stage，生成Task

#### saveAsTextFile

将数据以文本的形式保存到文件系统中，一个分区对应一个结果文件，可以指定hdfs文件系统，也可以指定本地文件系统（本地文件系统要写file://协议），数据的写入是由Executor中Task写入的，是多个Task并行写入的。

#### collect

每个分区对应的Task，将数据在Executor中，将数据以集合的形式保存到内存中，然后将每个分区对应的数据以数组形式通过网络**收集回Driver端**，数据按照分区编号有序返回

> 如果Driver的内存相对较小，并且每个分区对应的数据比较大，通过网络传输的数据，返回到Driver，当返回到Driver端的数据达到了一定大小，就不收集了，即将一部分无法收集的数据丢弃。如果需要将大量的数据收集到Driver端，那么可以在提交任务的时候指定Driver的内存大小 (--driver-memory 2g)

#### aggregate

方式是Action，可以将多个分区的数据进行聚合运算，例如进行相加，比较大小等

```scala
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 4)

//f1是在Executor端执行的
val f1 = (a: Int, b: Int) => {
  println("f1 function invoked ~~~~")
  a + b
}

//f2实在Driver端执行的
val f2 = (m: Int, n: Int) => {
  println("f2 function invoked !!!!")
  m + n
}

//返回的结果为55
val r1: Int = rdd1.aggregate(0)(f1, f2)
//返回的结果为50055
val r2: Int = rdd1.aggregate(10000)(f1, f2)
```

#### reduce

将数据先在每个分区内进行局部聚合，然后将每个分区返回的结果在Driver端进行全局聚合

```scala
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 4)
val f1 = (a: Int, b: Int) => {
  println("f1 function invoked ~~~~")
  a + b
}
//f1这个函数即在Executor中执行，又在Driver端执行
//reduce方法局部聚合的逻辑和全局聚合的逻辑是一样的
//局部聚合是在每个分区内完成（Executor）
//全局聚合实在Driver完成的
val r = rdd1.reduce(f1)
// f1 function invoked ~~~~
// f1 function invoked ~~~~
// f1 function invoked ~~~~
// 55
```

#### fold

fold跟reduce类似，只不过fold是一个柯里化方法，第一个参数可以指定一个初始值

```scala
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 4)
//fold与reduce方法类似，该方法是一个柯里化方法，第一个括号传入的初始值是0.0
//第二个括号传入的函数(_ + _) ，局部聚合和全局聚合都是相加
val r = rdd1.fold(0)(_ + _)
```

#### min、max

将整个RDD中全部对应的数据求最大值或最小值，底层的实现是：现在每个分区内求最大值或最小值，然后将每个分区返回的数据在Driver端再进行比较（min、max没有shuffle）

#### count

返回rdd元素的数量，先在每个分区内求数据的条数，然后再将每个分区返回的条数**在Driver进行求和**

#### take

返回一个由数据集的前n个元素组成的数组，即从RDD的0号分区开始取数据，**take可能触发一到多次Action（可能生成多个Job）**因为首先从0号分区取数据，如果取够了，就直接返回，没有取够，再触发Action，从后面的分区继续取数据，直到取够指定的条数为止

#### top

将RDD中数据按照降序或者指定的排序规则，返回前n个元素

```scala
val rdd1 = sc.parallelize(List(
  5, 7, 6, 4,
  9, 6, 1, 7,
  8, 2, 8, 5,
  4, 3, 10, 9
), 4)

val res1: Array[Int] = rdd1.top(2)
//指定排序规则，如果没有指定，使用默认的排序规则
implicit val ord = Ordering[Int].reverse
val res2: Array[Int] = rdd1.top(2)
val res3: Array[Int] = rdd1.top(2)(Ordering[Int].reverse)
```

> top底层调用的使用takeOrdered

#### takeOrdered

takeOrdered更灵活，可以传指定排序规则。底层是先在每个分区内求topN，然后将每个分区返回的结果再在Diver端求topN

> 在每个分区内进行排序，使用的是有界优先队列，特点是数据添加到其中，就会按照指定的排序规则排序，并且允许数据重复，最多只存放最大或最小的N个元素

#### foreachPartition

和foreach类似，只不过是以分区位单位，一个分区对应一个迭代器，应用外部传的函数，函数没有返回值，通常使用该方法将数据写入到外部存储系统中，一个分区获取一个连接，效率更高

### RDD 持久化

#### 1）RDD Cache 缓存

将数据缓存到内存（Executor的内存），第一次触发Action，才会将数据进行运算然后放入到内存，以后在触发Action，可以复用前面内存中缓存的数据，可以提升执行的效率。

cache和persist的使用场景：一个application多次触发Action，为了复用前面RDD计算好的数据，避免反复读取HDFS（数据源）中的数据和重复计算，可以将数据缓存到内存或磁盘【executor所在的磁盘】，第一次触发action才放入到内存或磁盘，以后会缓存的RDD进行操作可以复用缓存的数据。

一个RDD多次触发Action缓存才有意义，如果将数据缓存到内存，内存不够，以分区位单位，只缓存部分分区的数据，cache底层调用persist，可以指定更加丰富的存储基本，支持多种StageLevel，可以将数据序列化,默认放入内存使用的是java对象存储，但是占用空间大，优点速度快，也可以使用其他的序列化方式

#### 2）RDD CheckPoint 检查点

所谓的检查点其实就是通过将 RDD 中间结果写入磁盘，在中间阶段做检查的容错。

checkpoint使用场景：适合复杂的计算【机器学习、迭代计算】，为了避免中间结果数据丢失重复计算，可以将宝贵的中间结果保存到hdfs中，保证中间结果安全。

在调用rdd的checkpint方法之前，一定要指定checkpoint的目录sc.setCheckPointDir，指的HDFS存储目录，为保证中间结果安全，将数据保存到HDFS中

第一次触发Action，才做checkpoint，会额外触发一个job，这个job的目的就是将结果保存到HDFS中

> cache   不改变血缘依赖   数据存储在 内存 或者磁盘 
>
> checkpoint   改变血缘依赖  数据存储在 第三方数据库  HDFS  redis

### Spark 中的血缘

#### 1）RDD 的血缘关系

RDD 的 Lineage 会记录 RDD 的元数据信息和转换行为，当该 RDD 的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。

#### 2）宽依赖与窄依赖

宽依赖表示同一个父（上游）RDD 的 Partition 被多个子（下游）RDD 的 Partition 依赖，会引起 Shuffle，总结：宽依赖我们形象的比喻为多生。

窄依赖表示每一个父 (上游)RDD 的 Partition 最多被子（下游）RDD 的一个 Partition 使用，窄依赖我们形象的比喻为独生子女。

宽依赖和窄依赖。有 Shuffle 的是宽依赖。